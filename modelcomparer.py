import csv
import json
import os
import re
import ast
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from rich.console import Console
from rich.table import Table

# Initialize rich console for better output formatting
console = Console()

MODEL_PRICES = {
    "gpt-4o": 2.5,
    "gpt-4o-mini": 0.150,
    "claude-opus": 15.0,
    "claude-sonnet": 3.0,
    "claude-haiku": 0.8,
    "gemini-1.5-pro": 1.25,
    "gemini-2.0-flash": 0.1,
    "mistral-large": 2.0,
}

def calculate_price_performance(model_name: str, accuracy_score: float) -> float:
    price = MODEL_PRICES.get(model_name, 1.0)  
    if price == 0:
        return float('inf') 
    return accuracy_score / price

def extract_gold_viewpoints(filepath: str) -> List[str]:
    console.print(f"[bold blue]Loading gold standard viewpoints from {filepath}...[/bold blue]")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    viewpoints = []
    for line in content.strip().split('\n'):
        if line.strip().startswith('-'):
            viewpoint = line.strip()[1:].strip()  # Remove the dash and strip whitespace
            viewpoints.append(viewpoint)
    
    console.print(f"[green]✓ Found {len(viewpoints)} gold standard viewpoints[/green]")
    return viewpoints

def extract_model_viewpoints(filepath: str) -> Dict[str, List[str]]:
    console.print(f"[bold blue]Loading model responses from {filepath}...[/bold blue]")
    
    model_viewpoints = {}
    
    with open(filepath, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) >= 2:
                model_name = row[0]
                viewpoints_str = row[1]
                
                try:
                    viewpoints_list = ast.literal_eval(viewpoints_str)
                    viewpoints = []
                    for item in viewpoints_list:
                        if isinstance(item, str):
                            if not item.startswith("Document Identifier:") and not item.startswith("Model:"):
                                if not item.endswith(":"):  
                                    viewpoints.append(item)
                    
                    model_viewpoints[model_name] = viewpoints
                except (SyntaxError, ValueError):
                    try:
                        viewpoints = re.findall(r"'([^']*)'", viewpoints_str)
                        filtered_viewpoints = [vp for vp in viewpoints 
                                             if not vp.startswith("Document Identifier:") 
                                             and not vp.startswith("Model:")
                                             and not vp.endswith(":")
                                             ]
                        model_viewpoints[model_name] = filtered_viewpoints
                    except Exception as e:
                        console.print(f"[bold red]Error parsing viewpoints for {model_name}: {e}[/bold red]")
                        model_viewpoints[model_name] = []
    
    console.print(f"[green]✓ Found responses for {len(model_viewpoints)} models[/green]")
    return model_viewpoints

def evaluate_model(model_name: str, model_viewpoints: List[str], gold_viewpoints: List[str], llm: Any) -> Dict[str, Any]:
    console.print(f"[bold cyan]Evaluating model: {model_name}[/bold cyan]")
    
    gold_text = "\n".join([f"- {vp}" for vp in gold_viewpoints])
    model_text = "\n".join([f"- {vp}" for vp in model_viewpoints])
    
    prompt = ChatPromptTemplate.from_template("""
    You are an expert evaluator for AI viewpoint extraction systems. I need you to compare extracted viewpoints with the gold standard (ground truth) viewpoints and provide an accuracy assessment.

    # Gold Standard Viewpoints:
    {gold_viewpoints}

    # Model Extracted Viewpoints (Model: {model_name}):
    {model_viewpoints}

    Please analyze these viewpoints carefully and provide:
    1. Coverage (0-100): Does the model touch on each of the viewpoints from the gold standard?
    2. Disambiguity (0-100): How distinct are each of the viewpoints generated by the model (higher score for less overlapping viewpoints)?
    3. Clarity (0-100): Does each viewpoint get conveyed clearly for a human?
    4. Overall Accuracy Score (0-100): An average of the above scores.
    
    For each of the above metrics, provide a brief explanation of your scoring.
    
    Format your response as JSON:
    {{
      "coverage_score": X,
      "coverage_explanation": "...",
      "disambiguity_score": X,
      "disambiguity_explanation": "...",
      "clarity_score": X,
      "clarity_explanation": "...",
      "overall_accuracy_score": X,
      "overall_explanation": "..."
    }}
    """)
    
    formatted_prompt = prompt.format(
        gold_viewpoints=gold_text,
        model_name=model_name,
        model_viewpoints=model_text
    )
    
    response = llm.invoke(formatted_prompt)
    
    try:
        evaluation_results = json.loads(response.content)
        console.print(f"[green]✓ Successfully evaluated {model_name}[/green]")
        
        price = MODEL_PRICES.get(model_name.lower(), None)
        if price is not None:
            evaluation_results["price_per_1k_tokens"] = price
            overall_score = evaluation_results.get("overall_accuracy_score", 0)
            evaluation_results["price_performance_ratio"] = calculate_price_performance(model_name, overall_score)
            evaluation_results["price_explanation"] = f"Cost per 1K tokens: ${price}. Performance per dollar: {evaluation_results['price_performance_ratio']:.2f}"
        
        return evaluation_results
    except json.JSONDecodeError:
        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response.content)
            if json_match:
                json_text = json_match.group(1)
                evaluation_results = json.loads(json_text)
                console.print(f"[green]✓ Successfully evaluated {model_name} (extracted from markdown)[/green]")
                
                price = MODEL_PRICES.get(model_name.lower(), None)
                if price is not None:
                    evaluation_results["price_per_1k_tokens"] = price
                    overall_score = evaluation_results.get("overall_accuracy_score", 0)
                    evaluation_results["price_performance_ratio"] = calculate_price_performance(model_name, overall_score)
                    evaluation_results["price_explanation"] = f"Cost per 1K tokens: ${price}. Performance per dollar: {evaluation_results['price_performance_ratio']:.2f}"
                
                return evaluation_results
            
            json_match = re.search(r'{[\s\S]*}', response.content)
            if json_match:
                json_text = json_match.group(0)
                evaluation_results = json.loads(json_text)
                console.print(f"[green]✓ Successfully evaluated {model_name} (extracted from text)[/green]")
                
                price = MODEL_PRICES.get(model_name.lower(), None)
                if price is not None:
                    evaluation_results["price_per_1k_tokens"] = price
                    overall_score = evaluation_results.get("overall_accuracy_score", 0)
                    evaluation_results["price_performance_ratio"] = calculate_price_performance(model_name, overall_score)
                    evaluation_results["price_explanation"] = f"Cost per 1K tokens: ${price}. Performance per dollar: {evaluation_results['price_performance_ratio']:.2f}"
                
                return evaluation_results
            
            console.print(f"[bold red]Failed to parse JSON for {model_name}[/bold red]")
            return {
                "error": "Failed to parse JSON response",
                "raw_response": response.content
            }
        except Exception as e:
            console.print(f"[bold red]Error processing evaluation for {model_name}: {e}[/bold red]")
            return {
                "error": str(e),
                "raw_response": response.content
            }

def save_results(results: Dict[str, Dict[str, Any]], output_path: str) -> None:
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    
    console.print(f"[bold green]Results saved to {output_path}[/bold green]")

def print_summary(results: Dict[str, Dict[str, Any]]) -> None:
    table = Table(title="Model Viewpoint Extraction Accuracy Results")
    
    table.add_column("Model", style="cyan")
    table.add_column("Coverage", style="green")
    table.add_column("Disambiguity", style="green")
    table.add_column("Clarity", style="green")
    table.add_column("Overall", style="bold green")
    table.add_column("Price/1K", style="yellow")
    
    for model_name, evaluation in results.items():
        if "error" not in evaluation:
            coverage = evaluation.get("coverage_score", "N/A")
            disambiguity = evaluation.get("disambiguity_score", "N/A")
            clarity = evaluation.get("clarity_score", "N/A")
            overall = evaluation.get("overall_accuracy_score", "N/A")
            price = evaluation.get("price_per_1k_tokens", "N/A")
            
            table.add_row(
                model_name,
                str(coverage),
                str(disambiguity),
                str(clarity),
                str(overall),
                f"${price}" if price != "N/A" else "N/A",
            )
        else:
            table.add_row(model_name, "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR")
    
    console.print(table)
    
def main():
    gold_standard_path = "standard_cases/cases3.txt"
    model_responses_path = "responses.csv"
    output_path = "model_accuracy_results.json"
    
    load_dotenv()
    
    api_key = os.getenv("OPENAI_KEY")
    if not api_key:
        console.print("[bold red]OpenAI API key not found. Please set the OPENAI_KEY environment variable.[/bold red]")
        return
    
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        openai_api_key=api_key
    )
    
    gold_viewpoints = extract_gold_viewpoints(gold_standard_path)
    model_viewpoints = extract_model_viewpoints(model_responses_path)
    
    console.print("[bold]Evaluating accuracy using GPT-4o-mini...[/bold]")
    results = {}
    
    for model_name, viewpoints in model_viewpoints.items():
        results[model_name] = evaluate_model(model_name, viewpoints, gold_viewpoints, llm)
    
    save_results(results, output_path)
    
    print_summary(results)

if __name__ == "__main__":
    main()